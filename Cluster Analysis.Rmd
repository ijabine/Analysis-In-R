---
title: "Cluster Analysis"
author: "Illarion Jabine"
date: "06/12/2019"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Cluster Analysis in R 

In this document we will go through two classical cluster analysis methodes: hierarchical and  K-means. There are other cluster methodes which will be discussed in other documents.

## 1. Hierarchical Cluster 

```{r load, echo=TRUE, message=FALSE, warning=FALSE}
# let's load a dimonds dataset from ggplot2 
library(tidyverse)

data(diamonds)
summary(diamonds)
dim(diamonds)
str(diamonds)
```

Now if I try to run the hierarchical clustering on the full dataset I will get "Error: cannot allocate vector of size 10.8 Gb" error message. I simply don not have enough RAM on my PC. So I will just split the dataset into two. I will use simple random split.


```{r split dataset, echo=TRUE}
## [Random Split]
require(caret);
require(klaR);

percentage <-  80 / 100

## the sample size
smp_size <- floor(percentage * nrow(diamonds))

## set the seed to make your partition reproductible
set.seed(123)
trainIndex  <- sample(seq_len(nrow(diamonds)),size = smp_size)

traindata <- diamonds[trainIndex,]
testdata <- diamonds[-trainIndex,]
```

I will work with testdata dataset from now on.

!!! Very important pre-processing operations for clustering is scaling and normalization of the dataset.
We can use caret preProcess() function, which gives several options for data pre-processing:

* “BoxCox“: apply a Box–Cox transform, values must be non-zero and positive.
* “YeoJohnson“: apply a Yeo-Johnson transform, like a BoxCox, but values can be negative.
* “expoTrans“: apply a power transform like BoxCox and YeoJohnson.
* “zv“: remove attributes with a zero variance (all the same value).
* “nzv“: remove attributes with a near zero variance (close to the same value).
* “center“: subtract mean from values.
* “scale“: divide values by standard deviation.
* “range“: normalize values.
* “pca“: transform data to the principal components.
* “ica“: transform data to the independent components.
* “spatialSign“: project data onto a unit circle.

By combining center and scale methodes we can standardize the data. 
We need to exclude all categorical (factor) variables from distance calculation. So, meaning we need to exclude cut, color and clarity variables. However, here I will use R standard function scale():

```{r scaling data}
testdata <- testdata[-c(2,3,4)]
# let's check if there are no missing values
colSums(testdata)
# The code below is caret way to do scaling, but we will not use it here, just for my info only.
# testdata_transformed <- preProcess(testdata, method = c("center", "scale"))
testdata_transformed <- scale(testdata)

# Just to check the scaling
colMeans(testdata_transformed)
apply(testdata_transformed,2,sd)

```


Now we can calculate distance matrix. by default the system uses euclidean distance, but there are other distances available (just type ?dist): "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski".
euclidean:
Usual distance between the two vectors (2 norm aka L_2), sqrt(sum((x_i - y_i)^2)).

maximum:
Maximum distance between two components of x and y (supremum norm)

manhattan:
Absolute distance between the two vectors (1 norm aka L_1).

canberra:
sum(|x_i - y_i| / (|x_i| + |y_i|)). Terms with zero numerator and denominator are omitted from the sum and treated as if the values were missing.

This is intended for non-negative values (e.g., counts), in which case the denominator can be written in various equivalent ways; Originally, R used x_i + y_i, then from 1998 to 2017, |x_i + y_i|, and then the correct |x_i| + |y_i|.

binary:
(aka asymmetric binary): The vectors are regarded as binary bits, so non-zero elements are ‘on’ and zero elements are ‘off’. The distance is the proportion of bits in which only one is on amongst those in which at least one is on.

minkowski:
The p norm, the pth root of the sum of the pth powers of the differences of the components.

We will use euclidian disctance, as it's default we don't need to specify it.

```{r echo=TRUE}
dist_matrix <- dist(x = testdata_transformed)
```

Now we can use the distance matrix in the hierarchical cluster analysis. The way the hierarchical custrering algorith works follows the following logic:
1. Calculat the distance between every pair of points and store it in a distance matrix.
2. Assign every point in its own cluster.
3. Merge the closest pairs of points based on the distances from the distance matrix. At this step the amount of clusters goes down by 1.
4. Recomputes the distance between the new cluster and the old ones and stores them in a new distance matrix.
5. Repeat steps 2 and 3 until all the clusters are merged into one single cluster.


It is important to understand different methodes used to measure the distance between clusters. Then the algorithm takes the minimum distance to merge the clusters:
Ward's method
Single linkage - calculates the minimum distance between the clusters before merging.
Complete linkage - calculates the maximum distance between clusters before merging.
Average linkage - calculates the average distance between clusters before merging
McQuitty's method
Median linkage
Centroid linkage - finds centroid of two clusters and then calculates the distance between the two before merging

We will use default method "complete".
We will also produce two plots: dendogram and bi plot.

```{r cluster analysis, message=FALSE, warning=FALSE}

h_clust_res <- hclust(dist_matrix)

```
h_clust_res object of class "hclust" has been created.

Now let's create dendogram and bi plot.
For dendogram we will use standard "plot" (?plot) function, to generate bi plot we will use biplot and princomp functions (see help(biplot) and help(princomp)).:

```{r cluster analysis plots, message=FALSE, warning=FALSE}
plot(h_clust_res)

# PCA analysis. princomp performs a principal components analysis on the given numeric data matrix and returns the results as an object of class princomp
pca_result <- princomp(testdata_transformed)
pca_result
# Now we can print a biplot:
biplot(pca_result)
```
The good thing about hierarchical clustering is that you can choose whatever number of clusters you want by cutting (by an imaginary horizontal line) the dendrogram in order to create the desired number of clusters. To do that we will use cutree command. For example we want to have three clusters, well for that we need to look at the dendrogram, and find at what Height the horizontal line intersects it three times. In our case it's when heigh is 14:
```{r cutting  dendrogram, message=FALSE, warning=FALSE}
cut_tree <- cutree(h_clust_res, k = 3, h = 14)
# let see proportions:
table(cut_tree)
# cutree created an integer vector with clusters from 1 to 3.

# Now let's show these clusters and the horizontal line on the plot:
plot(h_clust_res)
rect.hclust(h_clust_res, k = 3, border = 2:6)
abline(h = 14, col = 'yellow')


```

Let's assign these clusters to our testdata, and print some cluster plots using ggplot2:

```{r}
hclust_results <- cbind(testdata,cut_tree)
hclust_results$cut_tree <- as.factor(hclust_results$cut_tree)
hclust_results %>% ggplot(aes(x = carat,y = price, color = cut_tree)) +
  geom_point()

```


## 2. K-means Clustering

! Important: I will use the dataset "testdata_transformed" from the hierarchical clustering.

The customer segmentation (clustering) is an unsupervised method that tries to find if hidden groups or structures exist in the data. We will use k-means clustering method with Euclidean distance metrics. First we will z-standardize the features for correct distance calculation. This normalization procedure is required to scale down variables to make sure that large variables do not dominate small ones. Then we will define the optimal number of clusters (k). kmeans() from stats package performs k-means clustering. K-means aims to partition the points into k groups such that the sum of squares from points to the assigned cluster centres is minimized. At the minimum, all cluster centres are at the mean of their Voronoi sets (the set of data points which are nearest to the cluster centre). It is a centroid-based clustering, whith the main idea of minimizing intra-cluster variation or total within-cluster variation to define clusters. The total within-cluster variation is calculated as the sum of squared Euclidean distances between points and the corresponding centroid: 
SSD = sum(Xi - Mk)^2

where: Xi is a point belonging to the cluster Ck
Mk - is the mean value of the points assigned to the cluster Ck
Each point Xk is assigned to a given cluster such that the sum of squared distance of the point to their assigned cluster centers Mk is minimized.

The optimization algorith works as follows:

1. Specify k - the number of clusters to be created.
2. Select randomly k objects from the dataset as the initial cluster centers.
3. Assign each observation to their closest centroid, based on the Euclidean distance between the object and the centroid.
4. For each of the k clusters recompute the cluster centroid by calculating the new mean value of all the data points in the cluster.
5. Iteratively minimize the total within sum of square or the total within-cluster variation. Repeat Step 3 and Step 4, until the centroids do not change or the maximum number of iterations is reached.

To use k-means we need first define the number of clusters k:

```{r Number of cluster calculation, echo=FALSE, message=FALSE, warning=FALSE}
library(factoextra)
fviz_nbclust(testdata_transformed,kmeans,method = "wss")
# Well the elbow seems to be around 2 or 3. So let's take k = 3

```

```{r k-means clustering, echo=FALSE, message=FALSE, warning=FALSE}

clustering.model <- kmeans(testdata_transformed, 3, nstart = 50)
# The object clustering.model of type kmeans has been created.
# Let's take the cluster vector from this object:
cluster_vector <- clustering.model$cluster
# Let's create a result dataset by combining the cluster vector with testdata

kmeans_results <- cbind(testdata,cluster_vector)
kmeans_results$cluster_vector <- as.factor(kmeans_results$cluster_vector)
kmeans_results %>% ggplot(aes(x = carat,y = price, color = cluster_vector)) +
  geom_point()

```
